{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Variables to save should be passed in a dict or a list: temp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-50415f624547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#Creates a saver object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m   1309\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m   1310\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m   1355\u001b[0m           \u001b[0mrestore_sequentially\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_sequentially\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m           build_save=build_save, build_restore=build_restore)\n\u001b[0m\u001b[1;32m   1358\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[0;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[1;32m    785\u001b[0m                        \" when eager execution is not enabled.\")\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m     \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ValidateAndSliceInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_ValidateAndSliceInputs\u001b[0;34m(self, names_to_saveables)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \"\"\"\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mnames_to_saveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseSaverBuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpListToDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mOpListToDict\u001b[0;34m(op_list, convert_variable_to_tensor)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m       raise TypeError(\"Variables to save should be passed in a dict or a \"\n\u001b[0;32m--> 555\u001b[0;31m                       \"list: %s\" % op_list)\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0;31m# When ResourceVariables are converted to Tensors, read ops are added to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;31m# graph. Sorting the op_list ensures that the resulting graph is always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Variables to save should be passed in a dict or a list: temp"
     ]
    }
   ],
   "source": [
    "\n",
    "cwd = os.getcwd()\n",
    "dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "#Project constants\n",
    "NR_CLASSES = 6\n",
    "\n",
    "\n",
    "#Control the graph being used by creating and using the \"with\" command. \n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    with tf.name_scope(\"placeholders\"):\n",
    "        x = tf.placeholder(tf.float32, (None, 200, 3))\n",
    "        y = tf.placeholder(tf.float32, (None, 6))\n",
    "        lr = tf.placeholder(tf.float32)\n",
    "          \n",
    "    with tf.name_scope(\"global_step\"):\n",
    "        # Creates a variable to hold the global_step. Global step is a variable that is updated every time training is started. \n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "              \n",
    "    with tf.name_scope(\"layers\"):\n",
    "        #Add any layers that constitute the network.\n",
    "        x_flat = tf.contrib.layers.flatten(x)\n",
    "        logits = tf.contrib.layers.fully_connected(x_flat,num_outputs = NR_CLASSES)#layer before softmax. \n",
    "        y_pred = tf.nn.softmax(logits, axis = 1)#last layer like a softmax. \n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #Define the loss function that needs to be minimized. \n",
    "        entropy = tf.losses.softmax_cross_entropy(onehot_labels= y,logits= logits,reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS) #if redctuion is None shape is [batch_size], otherwise it is reduced to a scalar. i.e. sum over all the samples.\n",
    "        l = tf.reduce_sum(entropy) #combined intropy of samples in batch. \n",
    "        \n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        # define the an optimizer object.\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(l, global_step=global_step) #global step counts the ammount of training epochs that has happened. \n",
    "        \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        #Value usefull when comparing probability vector with one-hot label : [0; 0.30; 0.70] vs [0; 0; 1]\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_pred, 1)) # check if largest value of label and prediction are the same. \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # calculate accuracy.\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"summary\"):\n",
    "        #Summary nodes( values that we want tot track ) \n",
    "        tf.summary.scalar('learning_rate', lr) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.scalar('loss', l)\n",
    "        tf.summary.scalar('global_step', global_step)\n",
    "        \n",
    "        #Creates an object that needs to be called in sess.run() to run all summary nodes. \n",
    "        create_summary_op = tf.summary.merge_all()\n",
    "        \n",
    "        #Write log to file for tensorboard. Start tensorboard by: tensorboard --logdir=path/to/log-director\n",
    "        logdir = ('log') ####### create variable for log file and graph)\n",
    "        filewriter = tf.summary.FileWriter(logdir,graph1)\n",
    "\n",
    "    with tf.name_scope(\"saver\"):\n",
    "        #Creates a saver object.\n",
    "        saver = tf.train.Saver('temp') \n",
    "        \n",
    "    with tf.name_scope(\"initializer\"):\n",
    "        #Create an initializer object to be called when running.\n",
    "        #If you are programming in the low-level TensorFlow API (that is, you are explicitly creating your own graphs and sessions), you must explicitly initialize the variables. Most high-level frameworks such as tf.contrib.slim, tf.estimator.Estimator and Keras automatically initialize variables for you before training a model.\n",
    "        init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7744516244750398411, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 79167488\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       device_id: 1\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 3299848771667172135\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\", name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 114819072\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 4041413039043931638\n",
       " physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session\n",
    "\"\"\"sess.run([fetch],{feed dict}): the fetch list determines the subgraphs to run in this session. The feeddict maps python datatypes to tf.tensors.\"\"\"\n",
    "def model_train(x_train, y_train, batch_size, nr_epochs, learning_rate, x_test = None, y_test = None):\n",
    "    \n",
    "    nr_samples = len(x_train)\n",
    "    nr_batches_in_dataset = nr_samples // batch_size\n",
    "     \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        for epoch_count in range(1,nr_epochs+1): \n",
    "            pos = 0\n",
    "            print(\"nr_epoch:\",epoch_count)\n",
    "            \n",
    "            #Initialize weigths\n",
    "            saver = tf.train.Saver()   \n",
    "            ### ?? tf.train.import_meta_graph('my_test_model-1000.meta')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(\"/temp\"))\n",
    "            \n",
    "            for step in range(1,nr_batches_in_dataset+1):\n",
    "                \n",
    "                #create batch data\n",
    "                x_batch= x_train[pos:pos+batch_size]\n",
    "                y_batch= y_train[pos:pos+batch_size]\n",
    "                \n",
    "                #Training\n",
    "                feed_dict = {x: x_batch, y: y_batch, lr: learning_rate} \n",
    "                _,summary,loss,acc=sess.run([train_op, create_summary_op, l, accuracy],feed_dict)\n",
    "                print(\"step %d, minibatch_loss %f, minibatch_accuracy %f\" %(step, loss, acc))\n",
    "                \n",
    "                #Increment to capture next batch\n",
    "                pos = pos + batch_size\n",
    "            \n",
    "            # Test on test dataset, if provided, at end of epoch. \n",
    "            if x_test is not None and y_test is not None: \n",
    "                feed_dict = {x: x_test, y: y_test} \n",
    "                epoch_test_acc=sess.run([accuracy],feed_dict)\n",
    "                print(\"Test Set Accuracy:\", epoch_test_acc)\n",
    "                \n",
    "            else: \n",
    "                print(\"No testing data provided.\")\n",
    "\n",
    "       \n",
    "        cwd = os.path.abspath(os.getcwd())\n",
    "        weight_save_path = cwd+'/temp_model'\n",
    "\n",
    "        save_path = saver.save(sess, weight_save_path, global_step= global_step, write_meta_graph=False)\n",
    "\n",
    "        print(\"No path was provided, weights not saved. Model saved int path: %s\"  % save_path)\n",
    "\n",
    "\n",
    "        # Print the global step. \n",
    "        print('global_step: %s' % tf.train.global_step(sess, global_step))\n",
    "        \n",
    "        # Parting message.\n",
    "        print(\"Training is finished\")\n",
    "        \n",
    "    \n",
    "        \n",
    "def model_predict(x_eval,graph = tf.get_default_graph()):\n",
    "\n",
    "    \"\"\" if weights are present, load them up.\"\"\"\n",
    "    sess = tf.Session(graph=graph) #deliberately start a session that is not closed because this allows to not have to save and reload the sessions weights every time we call this method. \n",
    "    #saver.restore(sess, weight_load_path)\n",
    "    print(\"Model restored.\")\n",
    "    y_eval = sess.run([y_pred], feed_dict = {x: x_eval})\n",
    "    return y_eval\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy dataset: \n",
    "k = np.transpose(np.array([range(0,200),range(0,200),range(0,200)]))\n",
    "p = np.array(range(0,6))\n",
    "\n",
    "x_train = np.array([k,k])\n",
    "y_train =  np.array([p,p])\n",
    "x_test = np.array([k,k])\n",
    "y_test = np.array([p,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy input parameters\n",
    "weight_save_path = os.getcwd()\n",
    "batch_size = 2\n",
    "nr_epochs = 30\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr_epoch: 1\n",
      "step 1, minibatch_loss 66.884766, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 2\n",
      "step 1, minibatch_loss 2118.448730, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 3\n",
      "step 1, minibatch_loss 2549.537109, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 4\n",
      "step 1, minibatch_loss 1928.696045, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 5\n",
      "step 1, minibatch_loss 913.901550, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 6\n",
      "step 1, minibatch_loss 1200.339355, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 7\n",
      "step 1, minibatch_loss 26.876392, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 8\n",
      "step 1, minibatch_loss 5370.652344, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 9\n",
      "step 1, minibatch_loss 2242.705566, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 10\n",
      "step 1, minibatch_loss 543.978210, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 11\n",
      "step 1, minibatch_loss 1312.248291, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 12\n",
      "step 1, minibatch_loss 1068.089600, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 13\n",
      "step 1, minibatch_loss 3891.016113, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 14\n",
      "step 1, minibatch_loss 1344.782593, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 15\n",
      "step 1, minibatch_loss 2034.095459, minibatch_accuracy 1.000000\n",
      "Test Set Accuracy: [1.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 16\n",
      "step 1, minibatch_loss 1583.240967, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 17\n",
      "step 1, minibatch_loss 800.325928, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 18\n",
      "step 1, minibatch_loss 3447.790039, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 19\n",
      "step 1, minibatch_loss 2488.793945, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 20\n",
      "step 1, minibatch_loss 4009.390869, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 21\n",
      "step 1, minibatch_loss 1867.650635, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 22\n",
      "step 1, minibatch_loss 2192.206787, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 23\n",
      "step 1, minibatch_loss 2571.363281, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 24\n",
      "step 1, minibatch_loss 1830.774902, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 25\n",
      "step 1, minibatch_loss 1733.181396, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 26\n",
      "step 1, minibatch_loss 4947.197266, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 27\n",
      "step 1, minibatch_loss 3249.653564, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 28\n",
      "step 1, minibatch_loss 1426.619507, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 29\n",
      "step 1, minibatch_loss 1889.416138, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "nr_epoch: 30\n",
      "step 1, minibatch_loss 547.324707, minibatch_accuracy 0.000000\n",
      "Test Set Accuracy: [0.0]\n",
      "--------------------------------------------------------------\n",
      "Model saved in path: /home/jgrietens/Documents/smartbeat/dl-track\n",
      "global_step: 1\n",
      "--------------------------------------------------------------\n",
      "Training is finished\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_train(x_train, y_train, batch_size = batch_size, nr_epochs=nr_epochs, learning_rate = learning_rate, x_test = x_test, y_test = y_test, weight_save_path = weight_save_path,graph = graph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fb71be9e4e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jgrietens/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 710, in __del__\n",
      "    if self._session is not None:\n",
      "AttributeError: 'Session' object has no attribute '_session'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-bb4cec6f2eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-a0780716fb42>\u001b[0m in \u001b[0;36mmodel_predict\u001b[0;34m(x_eval, graph)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m#saver.restore(sess, weight_load_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_eval\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1066\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "model_predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
