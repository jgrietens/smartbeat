{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorboard import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jgrietens/Documents/smartbeat/dl-track/2-0-simplified\n"
     ]
    }
   ],
   "source": [
    "dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jgrietens/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "#Project constants\n",
    "NR_CLASSES = 6\n",
    "\n",
    "#Reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Control the graph being used by creating and using the \"with\" command. \n",
    "graph1 = tf.Graph()\n",
    "graph1.as_default()\n",
    "    \n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32, (None, 200, 3), name = 'x')\n",
    "    y = tf.placeholder(tf.float32, (None, 6), name = 'y')\n",
    "    lr = tf.placeholder(tf.float32, name = 'lr')\n",
    "          \n",
    "with tf.name_scope(\"global_step\"):\n",
    "    # Creates a variable to hold the global_step. Global step is a variable that is updated every time training is started. \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "              \n",
    "with tf.name_scope(\"layers\"):\n",
    "    #Add any layers that constitute the network.\n",
    "    x_flat = tf.contrib.layers.flatten(x)\n",
    "    l1 = tf.contrib.layers.fully_connected(x_flat, num_outputs = 100)\n",
    "    l2 = tf.contrib.layers.fully_connected(l1, num_outputs = 50)\n",
    "    l3 = tf.contrib.layers.fully_connected(l2, num_outputs = 20)\n",
    "    \n",
    "    #Layer before softmax\n",
    "    logits = tf.contrib.layers.fully_connected(l3, num_outputs = NR_CLASSES) #layer before softmax. \n",
    "    \n",
    "    #Prediction layer\n",
    "    y_pred = tf.nn.softmax(logits, axis = 1)#last layer like a softmax. \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    #Define the loss function that needs to be minimized. \n",
    "    entropy = tf.losses.softmax_cross_entropy(onehot_labels= y,logits= logits,reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS) #if redctuion is None shape is [batch_size], otherwise it is reduced to a scalar. i.e. sum over all the samples.\n",
    "    l = tf.reduce_sum(entropy) #combined intropy of samples in batch. \n",
    "        \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    # define the an optimizer object.\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(l, global_step=global_step) #global step counts the ammount of training epochs that has happened. \n",
    "        \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    #Value usefull when comparing probability vector with one-hot label : [0; 0.30; 0.70] vs [0; 0; 1]\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_pred, 1)) # check if largest value of label and prediction are the same. \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # calculate accuracy.\n",
    "        \n",
    "        \n",
    "with tf.name_scope(\"summary\"):\n",
    "    #Summary nodes( values that we want tot track ) \n",
    "    tf.summary.scalar('learning_rate', lr) \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', l)\n",
    "    tf.summary.scalar('global_step', global_step)\n",
    "        \n",
    "    #Creates an object that needs to be called in sess.run() to run all summary nodes. \n",
    "    create_summary_op = tf.summary.merge_all()\n",
    "        \n",
    "with tf.name_scope(\"initializer\"):\n",
    "    #Create an initializer object to be called when running.\n",
    "    #If you are programming in the low-level TensorFlow API (that is, you are explicitly creating your own graphs and sessions), you must explicitly initialize the variables. Most high-level frameworks such as tf.contrib.slim, tf.estimator.Estimator and Keras automatically initialize variables for you before training a model.\n",
    "    init_op = tf.global_variables_initializer()\n",
    "        \n",
    "with tf.name_scope(\"saver\"):\n",
    "    #Creates a saver object.\n",
    "    saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Training/running definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session\n",
    "\"\"\"sess.run([fetch],{feed dict}): the fetch list determines the subgraphs to run in this session. The feeddict maps python datatypes to tf.tensors.\"\"\"\n",
    "def model_train(x_train, y_train, batch_size, nr_epochs, learning_rate, run_name, x_test = None, y_test = None):\n",
    "    \n",
    "    #Deterine nr. of batches in provided dataset. \n",
    "    nr_samples = len(x_train)\n",
    "    nr_batches_in_dataset = nr_samples // batch_size\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #Write log to file for tensorboard. Start tensorboard by: tensorboard --logdir=path/to/log-director\n",
    "        logdir = dir + \"/\" + run_name   ####### create variable for log file and graph)\n",
    "        filewriter = tf.summary.FileWriter(logdir, sess.graph)\n",
    "        \n",
    "        #Initialize weigths: last checkpoint or init_op.\n",
    "        \"\"\"Note: Shouldn't this whole if statement be integrated in init_op & the graph?\"\"\"\n",
    "        checkpoint = tf.train.get_checkpoint_state(dir + \"/\" + run_name  )\n",
    "        \n",
    "        if checkpoint is None:\n",
    "            print(\"No checkpoint was found, random initilization is started...\")\n",
    "            sess.run([init_op])\n",
    "        else:\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(dir + \"/\" + run_name ))\n",
    "        \n",
    "        #Go through all the different epochs\n",
    "        for epoch_count in range(1,nr_epochs+1): \n",
    "            pos = 0\n",
    "            print(\"EPOCH \",epoch_count)\n",
    "            \n",
    "            for step in range(1,nr_batches_in_dataset+1):\n",
    "\n",
    "                #Create batch data\n",
    "                x_batch= x_train[pos:pos+batch_size]\n",
    "                y_batch= y_train[pos:pos+batch_size]\n",
    "                \n",
    "                #Training\n",
    "                feed_dict = {x: x_batch, y: y_batch, lr: learning_rate} \n",
    "                _,loss,acc,summary =sess.run([train_op, l, accuracy, create_summary_op],feed_dict)\n",
    "                print(\"-batch %d, minibatch_loss %f\" %(step, loss))\n",
    "                \n",
    "                #Write summary to disk\n",
    "                filewriter.add_summary(summary, global_step = tf.train.global_step(sess, global_step))\n",
    "                filewriter.flush()\n",
    "                \n",
    "                #Increment to capture next batch\n",
    "                pos = pos + batch_size\n",
    "            \n",
    "            # Test on test dataset, if provided, at end of epoch. \n",
    "            if x_test is not None and y_test is not None: \n",
    "                feed_dict = {x: x_test, y: y_test} \n",
    "                epoch_test_acc=sess.run([accuracy],feed_dict)\n",
    "                print(\"Test Set Accuracy:\", epoch_test_acc)\n",
    "            else: \n",
    "                print(\"No testing data provided.\")\n",
    "\n",
    "        save_path = saver.save(sess, dir + \"/\" + run_name +\"/model\", global_step= global_step, write_meta_graph=False)\n",
    "\n",
    "        print(\"Weights saved in path: %s\"  % dir + \"/\" + run_name + \"/model\")\n",
    "        print(\"tb_events saved in path: %s\"  % filewriter.get_logdir())\n",
    "        \n",
    "\n",
    "        # Print the global step. \n",
    "        print('global_step: %s' % tf.train.global_step(sess, global_step))\n",
    "\n",
    "    \n",
    "        \n",
    "def model_predict(x_eval, graph = tf.get_default_graph()):\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Restore last saved checkpoint. \n",
    "        saver.restore(sess, tf.train.latest_checkpoint(dir + \"/\" + run_name + \"/model\"))\n",
    "        \n",
    "        print(\"Model restored.\")\n",
    "        \n",
    "        #Evaluate(infer) the function on given input\n",
    "        y_eval = sess.run([y_pred], feed_dict = {x: x_eval})\n",
    "        \n",
    "        return y_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy dataset: \n",
    "k = np.transpose(np.array([range(0,200),range(0,200),range(0,200)]))\n",
    "p = np.array(range(0,6))\n",
    "\n",
    "x_train = np.array([k,k])\n",
    "y_train =  np.array([p,p])\n",
    "x_test = np.array([k,k])\n",
    "y_test = np.array([p,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy input parameters\n",
    "weight_save_path = os.getcwd()\n",
    "batch_size = 1\n",
    "nr_epochs = 5\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/jgrietens/Documents/smartbeat/dl-track/2-0-simplified/test5/model-10\n",
      "EPOCH  1\n",
      "-batch 1, minibatch_loss 360810.375000\n",
      "-batch 2, minibatch_loss 457545.437500\n",
      "Test Set Accuracy: [1.0]\n",
      "EPOCH  2\n",
      "-batch 1, minibatch_loss 572609.375000\n",
      "-batch 2, minibatch_loss 723431.750000\n",
      "Test Set Accuracy: [1.0]\n",
      "EPOCH  3\n",
      "-batch 1, minibatch_loss 907978.250000\n",
      "-batch 2, minibatch_loss 1130180.250000\n",
      "Test Set Accuracy: [1.0]\n",
      "EPOCH  4\n",
      "-batch 1, minibatch_loss 1394630.500000\n",
      "-batch 2, minibatch_loss 1706412.500000\n",
      "Test Set Accuracy: [1.0]\n",
      "EPOCH  5\n",
      "-batch 1, minibatch_loss 2071092.500000\n",
      "-batch 2, minibatch_loss 2494736.000000\n",
      "Test Set Accuracy: [1.0]\n",
      "Weights saved in path: /home/jgrietens/Documents/smartbeat/dl-track/2-0-simplified/test5/model\n",
      "tb_events saved in path: /home/jgrietens/Documents/smartbeat/dl-track/2-0-simplified/test5\n",
      "global_step: 20\n"
     ]
    }
   ],
   "source": [
    "model_train(x_train, y_train, batch_size = batch_size, nr_epochs=nr_epochs, learning_rate = learning_rate, run_name= \"test5\", x_test = x_test, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
