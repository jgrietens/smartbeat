{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-81718c65ac97>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-81718c65ac97>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    logits = #layer before softmax.\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "#Graph\n",
    "\n",
    "#Control the graph being used by creating and using the \"with\" command. \n",
    "graph1 = tf.graph()\n",
    "with graph1.as_default():\n",
    "\twith tf.name_scope(\"placeholders\"):\n",
    "\t\tx = tf.placeholder(tf.float32, (None, ...))\n",
    "\t\ty = tf.placeholder(tf.float32, (None, ...))\n",
    "\t\tlr =  tf.placeholder(tf.float32)\n",
    "\t\t\n",
    "\t\t\n",
    "\twith tf.name_scope(\"layers\"):\n",
    "\t\t#Add any layers that constitute the network.\n",
    "\t\tlogits = #layer before softmax. \n",
    "\t\ty_pred = #last layer like a softmax. \n",
    "\t\t\n",
    "\twith tf.name_scope(\"initializer\"):\n",
    "\t\t#Create an initializer object to be called when running.\n",
    "\t\t#If you are programming in the low-level TensorFlow API (that is, you are explicitly creating your own graphs and sessions), you must explicitly initialize the variables. Most high-level frameworks such as tf.contrib.slim, tf.estimator.Estimator and Keras automatically initialize variables for you before training a model.\n",
    "\t\tinit_op = tf.global_variables_initializer()\n",
    "\t\t\n",
    "\twith tf.name_scope(\"loss\"):\n",
    "\t\t#Define the loss function that needs to be minimized. \n",
    "\t\tentropy = tf.losses.softmax_cross_entropy(onehot_labels,logits,reduction=Reduction.SUM_BY_NONZERO_WEIGHTS) #if redctuion is None shape is [batch_size], otherwise it is reduced to a scalar. i.e. sum over all the samples.\n",
    "\t\tl = tf.reduce_sum(entropy) #combined intropy of samples in batch. \n",
    "\t\t\n",
    "\twith tf.name_scope(\"optimizer\"):\n",
    "\t\t# define the an optimizer object.\n",
    "\t\ttrain_op = tf.train.AdamOptimizer(lr).minimize(l, global_step=global_step) #global step counts the ammount of training epochs that has happened. \n",
    "\t\t\n",
    "\twith tf.name_scope(\"accuracy\"):\n",
    "\t\t#Value usefull when comparing probability vector with one-hot label : [0; 0.30; 0.70] vs [0; 0; 1]\n",
    "\t\tcorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_pred, 1)) # check if largest value of label and prediction are the same. \n",
    "\t\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # calculate accuracy.\n",
    "\t\t\n",
    "\t\t\n",
    "\twith tf.name_scope(\"summary\"):\n",
    "\t\t#Summary nodes( values that we want tot track ) \n",
    "\t\ttf.summary.scalar('learning rate', lr) \n",
    "\t\ttf.summary.scalar('accuracy', accuracy)\n",
    "\t\ttf.summary.scalar('loss', l)\n",
    "\t\t\n",
    "\t\t#Creates an object that needs to be called in sess.run() to run all summary nodes. \n",
    "\t\tcreate_summary_op = tf.summary.merge_all()\n",
    "\t\t#Write log to file for tensorboard. Start tensorboard by: tensorboard --logdir=path/to/log-directory\n",
    "\t\tcurrent_path = pwd\n",
    "\t\tlogdir = (current_path + '/log') ####### create variable for log file and graph)\n",
    "\t\tfilewriter = tf.summary.Filewriter(logdir,graph1)\n",
    "\t\t\n",
    "\twith tf.name_scope(\"saver\"):\n",
    "\t\t#Creates a saver object.\n",
    "\t\tsaver = tf.train.Saver() \n",
    "\t\t\n",
    "\twith tf.name_scope(\"global_step\"):\n",
    "\t\t# Creates a variable to hold the global_step. Global step is a variable that is updated every time training is started. \n",
    "\t\tglobal_step = tf.Variable(0, trainable=False, name='global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Session\n",
    "\"\"\"sess.run([fetch],{feed dict}): the fetch list determines the subgraphs to run in this session. The feeddict maps python datatypes to tf.tensors.\"\"\"\n",
    "def model_train(graph = tf.get_default_graph ,x_train, y_train, batch_size, nr_epochs, learning_rate, x_test = None, y_test = None, weight_save_path = None):\n",
    "\tnr_samples = len(x_train)\n",
    "\tnr_batches_in_dataset = nr_sample // batch_size\n",
    "\t\n",
    "\tsess = tf.Session(graph,options = None) #deliberately start a session that is not closed because this allows to not have to save and reload the sessions weights every time we call this method. \n",
    "\t\n",
    "\tfor epoch_count in range(1,nr_epochs): \n",
    "\t\tpos = 0\n",
    "\t\tfor step in range(1,nr_batches_in_dataset)\n",
    "\t\t\t#create batch data\n",
    "\t\t\tx_batch= x_train[pos:pos+batch_size]\n",
    "\t\t\ty_batch= y_train[pos:pos+batch_size]\n",
    "\n",
    "\t\t\t\t#Training\n",
    "\t\t\t\t\"\"\"During training for each minibatch is Minitbatch_Loss & Training accuracy are printed. If a test set is provided, the testing accuracy is calculated at the end of each epoch.\"\"\"\n",
    "\t\t\t\tif train: #Whilst training. \n",
    "\t\t\t\t\tsess.run(init_op) #initialize all variables if training is on. \n",
    "\t\t\t\t\tfeed_dict = {x: x_batch, y: y_batch, lr: learning_rate} ### create method variables for these values. Be carefull not to use same names as in this example. \n",
    "\t\t\t\t\t_,summary,loss,acc,_=sess.run([train_op, create_summary_op, l, accuracy, filewriter],feed_dict) ### is  ceate_sumary_op not running l, accuracy already ? \n",
    "\t\t\t\t\tprint(\"step %d, training_loss %f, training_accuracy %f\" %(i, loss, acc) #print the values to the shell.\n",
    "\t\t\t\t\tprint(\"Training is finished\")\n",
    "\t\t\t\t\t# Calculate at the end of the epoch: on x_test, y_test the accuracy. \n",
    "\t\t\tpos = pos + batch_size\n",
    "\t\t\t\n",
    "\t\t\"\"\"End-of-epoch test set calculation.\"\"\"\n",
    "\t\tif (x_test and y_test) is not None:\t \n",
    "\t\t# Checks if data is present. And calculates the Testing accuracy if present.\n",
    "\t\t\t### Do we need to load values from model in some way????\n",
    "\t\t\tfeed_dict = {x: x_test, y: y_test} ### create method variables for these values. Be carefull not to use same names as in this example. \n",
    "\t\t\tepoch_test_acc,_=sess.run([accuracy, filewriter],feed_dict)\n",
    "\t\t\tprint(\"nr_epoch:\",epoch_count ,\"|| Testing Accuracy:\", epoch_test_acc)\n",
    "\t\telse: \n",
    "\t\t\tprint(\"No testing data was provided, can't calculate Testing Accuracy at the end of the epoch\")\n",
    "\t\t\t\n",
    "\tif weight_save_path is not None:\n",
    "\t\t\"\"\"There was a weight_save_path provided and we will write to it.\"\"\"\n",
    "\t\t# Save the variables to disk.\n",
    "\t\tsave_path = saver.save(sess, weight_save_path, global_step= global_step, ,write_meta_graph=False)\n",
    "\t\tprint(\"Model saved in path: %s\" % os.path.abspath(weight_save_path))\n",
    "\t\t\n",
    "\telse:\n",
    "\t\t\"\"\"There was no save_path provided. We need to create one. \"\"\"\n",
    "\t\tcwd = os.path.abspath(os.getcwd())\n",
    "\t\tweight_save_path = cwd+'/temp_model'\n",
    "\t\t\n",
    "\t\tsave_path = saver.save(sess, weight_save_path, global_step= global_step ,write_meta_graph=False)\n",
    " )\n",
    "\t\tprint(\"No path was provided, weights not saved. Model saved int path: %s\"  % save_path)\n",
    "\t\t\n",
    "\t\n",
    "\t# Print the global step. \n",
    "\tprint('global_step: %s' % tf.train.global_step(sess, global_step_tensor))\n",
    "\t# Parting message.\n",
    "\tprint(\"don't forget to close session if a new sessions is to be started with the command sess.close()\")\n",
    "\t \n",
    "\t\t\n",
    "def model_predict(graph = tf.get_default_graph, x_eval, weight_load_path):\n",
    "\n",
    "\t\"\"\" if weights are present, load them up.\"\"\"\n",
    "\tsess = tf.Session(graph,options = None) #deliberately start a session that is not closed because this allows to not have to save and reload the sessions weights every time we call this method. \n",
    "\tsaver.restore(sess, weight_load_path)\n",
    "\tprint(\"Model restored.\")\n",
    "\t\ty_eval = sess.run([y_pred], feed_dict{x: x_eval})\n",
    "\treturn y_eval\n",
    "\t\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
