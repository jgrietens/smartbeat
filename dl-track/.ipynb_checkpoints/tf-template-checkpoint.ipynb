{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#Project constants\n",
    "NR_CLASSES = 6\n",
    "\n",
    "\n",
    "\n",
    "#Control the graph being used by creating and using the \"with\" command. \n",
    "graph1 = tf.Graph()\n",
    "with graph1.as_default():\n",
    "    with tf.name_scope(\"placeholders\"):\n",
    "        x = tf.placeholder(tf.float32, (None, 200, 3))\n",
    "        y = tf.placeholder(tf.float32, (None, 6))\n",
    "        lr = tf.placeholder(tf.float32)\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"global_step\"):\n",
    "        # Creates a variable to hold the global_step. Global step is a variable that is updated every time training is started. \n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "              \n",
    "    with tf.name_scope(\"layers\"):\n",
    "        #Add any layers that constitute the network.\n",
    "        x_flat = tf.contrib.layers.flatten(x)\n",
    "        logits = tf.contrib.layers.fully_connected(x_flat,num_outputs = NR_CLASSES)#layer before softmax. \n",
    "        y_pred = tf.nn.softmax(logits, axis = 1)#last layer like a softmax. \n",
    "        \n",
    "    with tf.name_scope(\"initializer\"):\n",
    "        #Create an initializer object to be called when running.\n",
    "        #If you are programming in the low-level TensorFlow API (that is, you are explicitly creating your own graphs and sessions), you must explicitly initialize the variables. Most high-level frameworks such as tf.contrib.slim, tf.estimator.Estimator and Keras automatically initialize variables for you before training a model.\n",
    "        init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        #Define the loss function that needs to be minimized. \n",
    "        entropy = tf.losses.softmax_cross_entropy(onehot_labels= y,logits= logits,reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS) #if redctuion is None shape is [batch_size], otherwise it is reduced to a scalar. i.e. sum over all the samples.\n",
    "        l = tf.reduce_sum(entropy) #combined intropy of samples in batch. \n",
    "        \n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        # define the an optimizer object.\n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(l, global_step=global_step) #global step counts the ammount of training epochs that has happened. \n",
    "        \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        #Value usefull when comparing probability vector with one-hot label : [0; 0.30; 0.70] vs [0; 0; 1]\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_pred, 1)) # check if largest value of label and prediction are the same. \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # calculate accuracy.\n",
    "        \n",
    "        \n",
    "    with tf.name_scope(\"summary\"):\n",
    "        #Summary nodes( values that we want tot track ) \n",
    "        tf.summary.scalar('learning_rate', lr) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.scalar('loss', l)\n",
    "        \n",
    "        #Creates an object that needs to be called in sess.run() to run all summary nodes. \n",
    "        create_summary_op = tf.summary.merge_all()\n",
    "        #Write log to file for tensorboard. Start tensorboard by: tensorboard --logdir=path/to/log-directory\n",
    "        current_path =  os.path.abspath(os.getcwd())\n",
    "        logdir = (current_path + '/log') ####### create variable for log file and graph)\n",
    "        #filewriter = tf.summary.FileWriter(logdir,graph1)\n",
    "\n",
    "    with tf.name_scope(\"saver\"):\n",
    "        #Creates a saver object.\n",
    "        saver = tf.train.Saver() \n",
    "        \n",
    "    with tf.name_scope(\"global_step\"):\n",
    "        # Creates a variable to hold the global_step. Global step is a variable that is updated every time training is started. \n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        \n",
    "    with tf.name_scope(\"initializer\"):\n",
    "        #Create an initializer object to be called when running.\n",
    "        #If you are programming in the low-level TensorFlow API (that is, you are explicitly creating your own graphs and sessions), you must explicitly initialize the variables. Most high-level frameworks such as tf.contrib.slim, tf.estimator.Estimator and Keras automatically initialize variables for you before training a model.\n",
    "        init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-286-fcef83125572>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-286-fcef83125572>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    if x_test and if y_test:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Session\n",
    "\"\"\"sess.run([fetch],{feed dict}): the fetch list determines the subgraphs to run in this session. The feeddict maps python datatypes to tf.tensors.\"\"\"\n",
    "def model_train(x_train, y_train, batch_size, nr_epochs, learning_rate, graph = tf.get_default_graph, x_test = None, y_test = None, weight_save_path = None):\n",
    "    \n",
    "    nr_samples = len(x_train)\n",
    "    nr_batches_in_dataset = nr_samples // batch_size\n",
    "    \n",
    "    sess = tf.Session(graph= graph) #deliberately start a session that is not closed because this allows to not have to save and reload the sessions weights every time we call this method. \n",
    "    \n",
    "    for epoch_count in range(1,nr_epochs+1): \n",
    "        pos = 0\n",
    "        for step in range(1,nr_batches_in_dataset+1):\n",
    "            #create batch data\n",
    "            x_batch= x_train[pos:pos+batch_size]\n",
    "            y_batch= y_train[pos:pos+batch_size]\n",
    "            #Training\n",
    "            \"\"\"During training for each minibatch is Minitbatch_Loss & Training accuracy are printed. If a test set is provided, the testing accuracy is calculated at the end of each epoch.\"\"\"\n",
    "            sess.run(init_op) #initialize all variables if training is on. \n",
    "            \n",
    "\n",
    "            feed_dict = {x: x_batch, y: y_batch, lr: learning_rate} ### create method variables for these values. Be carefull not to use same names as in this example. \n",
    "            ###deleted filewriter in below fetch list. \n",
    "            _,_,summary,loss,acc=sess.run([train_op, init_op, create_summary_op, l, accuracy],feed_dict) ### is  ceate_sumary_op not running l, accuracy already ? \n",
    "            \n",
    "            ###print(type(step),type(loss),type(acc))\n",
    "            \n",
    "            print(\"step %d, training_loss %f, training_accuracy %f\" %(step, loss, acc)) #print the values to the shell.\n",
    "            print(\"Training is finished\")\n",
    "            # Calculate at the end of the epoch: on x_test, y_test the accuracy. \n",
    "            pos = pos + batch_size\n",
    "\n",
    "        \"\"\"End-of-epoch test set calculation.\"\"\"\n",
    "        if x_test and y_test: \n",
    "        # Checks if data is present. And calculates the Testing accuracy if present.\n",
    "            ### Do we need to load values from model in some way????\n",
    "            feed_dict = {x: x_test, y: y_test} ### create method variables for these values. Be carefull not to use same names as in this example. \n",
    "            epoch_test_acc,_=sess.run([accuracy, filewriter],feed_dict)\n",
    "            print(\"nr_epoch:\",epoch_count ,\"|| Testing Accuracy:\", epoch_test_acc)\n",
    "        else: \n",
    "            print(\"No testing data was provided, can't calculate Testing Accuracy at the end of the epoch\")\n",
    "\n",
    "    if weight_save_path is not None:\n",
    "        \"\"\"There was a weight_save_path provided and we will write to it.\"\"\"\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(sess, weight_save_path, global_step= global_step, write_meta_graph=False)\n",
    "        print(\"Model saved in path: %s\" % os.path.abspath(weight_save_path))\n",
    "        \n",
    "    else:\n",
    "        \"\"\"There was no save_path provided. We need to create one. \"\"\"\n",
    "        cwd = os.path.abspath(os.getcwd())\n",
    "        weight_save_path = cwd+'/temp_model'\n",
    "        \n",
    "        save_path = saver.save(sess, weight_save_path, global_step= global_step, write_meta_graph=False)\n",
    " \n",
    "        print(\"No path was provided, weights not saved. Model saved int path: %s\"  % save_path)\n",
    "    \n",
    "\n",
    "    # Print the global step. \n",
    "    print('global_step: %s' % tf.train.global_step(sess, global_step_tensor))\n",
    "    # Parting message.\n",
    "    print(\"don't forget to close session if a new sessions is to be started with the command sess.close()\")\n",
    "     \n",
    "        \n",
    "def model_predict(x_eval, weight_load_path, graph = tf.get_default_graph):\n",
    "\n",
    "    \"\"\" if weights are present, load them up.\"\"\"\n",
    "    sess = tf.Session(graph) #deliberately start a session that is not closed because this allows to not have to save and reload the sessions weights every time we call this method. \n",
    "    saver.restore(sess, weight_load_path)\n",
    "    print(\"Model restored.\")\n",
    "    y_eval = sess.run([y_pred], feed_dict = {x: x_eval})\n",
    "    return y_eval\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy dataset: \n",
    "x_train1 = np.array([np.transpose(np.array([range(0,200),range(0,200),range(0,200)]))])\n",
    "y_train1 = np.array([np.array(range(0,6))])\n",
    "x_test1 = np.array([np.transpose(np.array([range(0,200),range(0,200),range(0,200)]))])\n",
    "y_test1 = np.array([np.array(range(0,6))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy input parameters\n",
    "weight_save_path = os.getcwd()\n",
    "batch_size = 1 \n",
    "nr_epochs = 1 \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, training_loss 1487.117798, training_accuracy 0.000000\n",
      "Training is finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-ce12d7051b3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnr_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-281-6e0be8c99453>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(x_train, y_train, batch_size, nr_epochs, learning_rate, graph, x_test, y_test, weight_save_path)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"\"\"End-of-epoch test set calculation.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Checks if data is present. And calculates the Testing accuracy if present.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m### Do we need to load values from model in some way????\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "model_train(x_train1, y_train1, batch_size = batch_size, nr_epochs=nr_epochs, learning_rate = learning_rate, graph = graph1, x_test = x_test1, y_test = y_test1, weight_save_path = weight_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joe': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199])}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joe\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-280-95806c26c887>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-280-95806c26c887>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if a\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
