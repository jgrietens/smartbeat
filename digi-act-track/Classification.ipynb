{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/WISDM_ar_v1.1_raw.txt', header=None, names=['user', 'label', 'timestamp', 'x', 'y', 'z'], comment=';')\n",
    "df = df.sort_values('timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The sampling rate to use\n",
    "SAMPLING_RATE = 20.0\n",
    "\n",
    "# Window length to use for classification (Samples @ 10 Hz)\n",
    "WINDOW_LENGTH = 100 \n",
    "\n",
    "# Stride length to use between windows (decrease to increase dataset size)\n",
    "WINDOW_STRIDE = WINDOW_LENGTH * 5\n",
    "\n",
    "# Mapping from label index to label name\n",
    "IDX_TO_LABELS = df.label.unique()\n",
    "\n",
    "# Mapping from label name to label index\n",
    "LABELS_TO_IDX = {label:idx for idx, label in enumerate(IDX_TO_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import sliding_window, split_user_df\n",
    "from preprocessing import resample_df\n",
    "\n",
    "# Split the label DataFrame per user\n",
    "users = df.groupby(('label', 'user'))\n",
    "\n",
    "wdws = []\n",
    "y = []\n",
    "ids = []\n",
    "for (label, user), user_df in users:\n",
    "    \n",
    "    # Split the user DataFrame per recording session\n",
    "    splits = split_user_df(user_df)\n",
    "    \n",
    "    for split_df in splits:\n",
    "\n",
    "        # Calculate the timestamp index in seconds\n",
    "        ts =(split_df.timestamp - split_df.timestamp.iloc[0]) / 1e9\n",
    "\n",
    "        if ts.iloc[-1] < 10:\n",
    "            # Skip sessions that are smaller than 10 seconds\n",
    "            continue\n",
    "            \n",
    "        # Resample the DataFrame to SAMPLING_RATE\n",
    "        split_df = resample_df(split_df, SAMPLING_RATE)\n",
    "        \n",
    "        # Calculate sliding windows\n",
    "        for i in xrange(0, len(split_df) - WINDOW_LENGTH, WINDOW_STRIDE):\n",
    "            wdws.append(split_df.iloc[i:i+WINDOW_LENGTH])\n",
    "            y.append(LABELS_TO_IDX[label])\n",
    "            ids.append(int(user))\n",
    "         \n",
    "        \n",
    "y = np.asarray(y)\n",
    "ids = np.asarray(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "\n",
    "\n",
    "DATA_COLUMNS = ['x', 'y', 'z']\n",
    "\n",
    "X = []\n",
    "\n",
    "for wdw_df in wdws:\n",
    "    # Calculate PCA for window\n",
    "    pca_values = PCA(len(DATA_COLUMNS)).fit_transform(wdw_df.loc[:, DATA_COLUMNS].fillna(0))\n",
    "    pca_wdw_df = pd.DataFrame(pca_values, columns=DATA_COLUMNS)\n",
    "    \n",
    "    # calculate FFT for axis X\n",
    "    ffts_x = np.abs(np.fft.fft(pca_wdw_df.x.values))\n",
    "    ffts_x = ffts_x[1:ffts_x.shape[-1]/2 + 1]\n",
    "    \n",
    "    ffts_y = np.abs(np.fft.fft(pca_wdw_df.y.values))\n",
    "    ffts_y = ffts_y[1:ffts_y.shape[-1]/2 + 1]\n",
    "    \n",
    "    ffts_z = np.abs(np.fft.fft(pca_wdw_df.z.values))\n",
    "    ffts_z = ffts_x[1:ffts_z.shape[-1]/2 + 1]\n",
    "    \n",
    "    #calculate the mean of the amplitudes of the vectors\n",
    "    vector_ampli = 0.0\n",
    "    x1=0.0\n",
    "    y1=0.0\n",
    "    z1=0.0\n",
    "\n",
    "    \n",
    "    for i in range(0,len(wdw_df.index)):\n",
    "              #print('-------------')\n",
    "              #print(i)\n",
    "              x1 = wdw_df.x.iloc[i]**2\n",
    "              #print(x)\n",
    "              y1 = wdw_df.y.iloc[i]**2\n",
    "              #print(y)\n",
    "              z1= wdw_df.z.iloc[i]**2\n",
    "              #print(z)\n",
    "              vector_ampli1= math.sqrt(x1+y1+z1)\n",
    "              vector_ampli += vector_ampli1\n",
    "              \n",
    "              #print('---')\n",
    "              #print(vector_ampli)\n",
    "\n",
    "              #print(vector_ampli1)\n",
    "              #print(vector_ampli)\n",
    "          \n",
    "    \n",
    "    vector_ampli_mean=np.float64(vector_ampli/(len(wdw_df.index)))\n",
    "    \n",
    "    \n",
    "    # Calculate features #wdws is a list of dataframes. \n",
    "    features = np.array([\n",
    "        wdw_df.x.mean(), # Orientation features\n",
    "        wdw_df.y.mean(), # Orientation features\n",
    "        wdw_df.z.mean(), # Orientation features\n",
    "        \n",
    "        #1\n",
    "        #wdw_df.x.std(),\n",
    "        #wdw_df.y.std(),\n",
    "        #wdw_df.z.std(),\n",
    "        \n",
    "        wdw_df.x.diff(periods=2).mean(),\n",
    "        wdw_df.y.diff(periods=2).mean(),\n",
    "        wdw_df.x.diff(periods=2).mean(),\n",
    "        ffts_x.mean(),\n",
    "        ffts_y.mean(),\n",
    "        ffts_z.mean(),\n",
    "        ffts_x.max(),\n",
    "        ffts_y.max(),\n",
    "        ffts_z.mean(),\n",
    "        \n",
    "        \n",
    "        #2\n",
    "        vector_ampli_mean,\n",
    "        \n",
    "        pca_wdw_df.x.std(),\n",
    "        pca_wdw_df.y.std(),\n",
    "        pca_wdw_df.z.std(),\n",
    "\n",
    "    ])\n",
    "    #print(type(wdw_df.x.mean()))\n",
    "    #print(type(vector_ampli_mean))\n",
    "    # Concatenate fft X with features\n",
    "    features = np.concatenate((features, ffts_x))\n",
    "    \n",
    "    X.append(features)\n",
    "    \n",
    "X = np.nan_to_num(np.asarray(X), 0)    \n",
    "print(type(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Walking  Jogging  Upstairs  Downstairs  Standing  Sitting\n",
      "Walking        90.3      4.3       2.3         3.0       0.0      0.0\n",
      "Jogging         1.2     96.8       1.3         0.7       0.0      0.0\n",
      "Upstairs       33.9     13.0      45.8         6.5       0.7      0.0\n",
      "Downstairs     46.3      2.6      13.0        38.1       0.0      0.0\n",
      "Standing        0.0      0.0       1.9         0.0      97.2      0.9\n",
      "Sitting         0.0      0.0       2.4         0.0       8.7     89.0\n",
      "\n",
      "Accuracy: 76.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Fix random values\n",
    "random.seed = 1234\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Create Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=40)\n",
    "\n",
    "\n",
    "# Apply cross validation on the users\n",
    "nr_folds = 5\n",
    "uniq_ids = np.unique(ids)\n",
    "kf = KFold(n_splits=nr_folds, shuffle=True)\n",
    "splits = kf.split(uniq_ids)\n",
    "    \n",
    "cm = np.zeros([len(LABELS_TO_IDX), len(LABELS_TO_IDX)])\n",
    "for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "    # Determine train & test user indices for this fold\n",
    "    train_idx = np.in1d(ids, uniq_ids[train_idx])\n",
    "    test_idx = np.in1d(ids, uniq_ids[test_idx])\n",
    "\n",
    "    # Determine train & test window indices for this fold\n",
    "\n",
    "    train_X, train_y = X[train_idx], y[train_idx]\n",
    "    test_X, test_y = X[test_idx], y[test_idx]\n",
    "\n",
    "    # Train the model on train set\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    # Perform prediction on test set\n",
    "    pred_y = model.predict(test_X)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm += confusion_matrix(y_true=test_y, y_pred=pred_y)\n",
    "\n",
    "\n",
    "# Calculate the mean of the Confusion Matrix\n",
    "cm = cm / np.float(nr_folds)\n",
    "\n",
    "# normalize Confusion matrix to 1 per class\n",
    "cm = cm / np.sum(cm, axis=1).reshape(-1, 1).astype(float)\n",
    "accuracy = cm.diagonal().sum() / len(cm)\n",
    "\n",
    "# convert Confusion Matrix to DataFrame\n",
    "cm = pd.DataFrame((100*cm).round(1))\n",
    "cm.columns = IDX_TO_LABELS\n",
    "cm.index = IDX_TO_LABELS\n",
    "\n",
    "print cm\n",
    "print\n",
    "print \"Accuracy: {}%\".format((accuracy * 100).round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
